version: '3.8'

services:
  # Main backend service: LocalAI + FastAPI
  backend:
    build: 
      context: ./backend
      dockerfile: Dockerfile
    env_file:
      - .env
    ports:
      - "${MAIN_API_PORT:-8080}:${API_PORT:-8000}"    # FastAPI
    environment:
      - LOCALAI_HOST=${LOCALAI_HOST:-localai}
      - LOCALAI_PORT=${LOCALAI_PORT:-8080}
      - DEFAULT_MODEL=${DEFAULT_MODEL:-llama3.2:1b}
      - API_TIMEOUT=${API_TIMEOUT:-300}
      - LOG_LEVEL=${LOG_LEVEL:-INFO}
    volumes:
      - ./backend/scripts:/app/scripts:ro
    depends_on:
      - localai
    command: ["api"]
    restart: unless-stopped
    networks:
      - llm-network

  # LocalAI service
  localai:
    image: localai/localai:latest-aio-gpu-nvidia-cuda-12
    env_file:
      - .env
    ports:
      - "${LOCALAI_PORT:-8080}:8080"
    environment:
      - MODELS_PATH=/models
      - THREADS=${LOCALAI_THREADS:-4}
      - CONTEXT_SIZE=${LOCALAI_CONTEXT_SIZE:-2048}
    volumes:
      - ${LOCALAI_MODELS_PATH:-./models}:/models
    restart: unless-stopped
    networks:
      - llm-network
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

networks:
  llm-network:
    driver: bridge