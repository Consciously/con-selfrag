# =============================================================================
# CON-LLM-CONTAINER ENVIRONMENT CONFIGURATION
# =============================================================================
# Copy this file to .env and customize the values for your environment
# This configuration supports both frontend and backend services

# =============================================================================
# BACKEND CONFIGURATION (FastAPI + LocalAI)
# =============================================================================

# LocalAI Configuration
# ---------------------
# Host and port for LocalAI service (internal container communication)
LOCALAI_HOST=0.0.0.0
LOCALAI_PORT=8080

# Default model to use (must be available in LocalAI)
# Popular options: llama-3.2-1b-instruct, llama-3.2-3b-instruct, codellama-7b-instruct
DEFAULT_MODEL=llama-3.2-1b-instruct

# GPU Configuration
# -----------------
# Enable debug mode for LocalAI (useful for GPU troubleshooting)
DEBUG=false

# API Configuration
# -----------------
# Internal API port (inside container)
API_PORT=8000

# External port mappings for different services
MAIN_API_PORT=8080          # Main backend service (FastAPI + LocalAI)
MAIN_LOCALAI_PORT=8080      # Main LocalAI port
DEV_API_PORT=8081           # Development backend service
DEV_LOCALAI_PORT=8081       # Development LocalAI port
LOCALAI_ONLY_PORT=8082      # LocalAI-only service port

# Backend Performance & Logging
# ------------------------------
# API request timeout in seconds
API_TIMEOUT=300

# Logging level: DEBUG, INFO, WARNING, ERROR, CRITICAL
LOG_LEVEL=INFO

# =============================================================================
# DOCKER VOLUME CONFIGURATION
# =============================================================================

# Volume Paths (customize for your setup)
# ----------------------------------------
# Model storage location (large files - LLM models)
# Default: Docker volume managed by LocalAI
LOCALAI_MODELS_PATH=/models

# Application logs location (runtime logs, debugging info)
# Default: ./logs (relative to docker-compose.yml)
LOGS_PATH=./logs

# User data and artifacts location (prompts, temp files, uploads)
# Default: ./data (relative to docker-compose.yml)
DATA_PATH=./data

# Log Configuration
# -----------------
# Log rotation: daily, weekly, size-based
LOG_ROTATION=daily

# Maximum log file size before rotation
LOG_MAX_SIZE=100MB

# Log retention period (days)
LOG_RETENTION_DAYS=30

# =============================================================================
# FRONTEND CONFIGURATION (Next.js)
# =============================================================================

# Frontend Ports
# --------------
FRONTEND_PORT=3000          # Production frontend port
FRONTEND_DEV_PORT=3001      # Development frontend port

# Backend Connection
# ------------------
# URL for frontend to connect to backend API
# In Docker: http://backend:8000
# For local development: http://localhost:8080
BACKEND_URL=http://backend:8000

# Next.js Configuration
# ---------------------
# Environment mode: development, production, test
NODE_ENV=production

# Disable Next.js telemetry (optional)
NEXT_TELEMETRY_DISABLED=1

# =============================================================================
# STORAGE CONFIGURATION
# =============================================================================

# Model Storage Path
# ------------------
# Local path where LocalAI models will be stored
# This should be a persistent directory with sufficient space (models can be several GB)
LOCALAI_MODELS_PATH=/mnt/assets/models/llm

# =============================================================================
# DOCKER COMPOSE PROFILES
# =============================================================================
# These are used internally by Docker Compose to manage service groups
# You don't need to modify these unless you're customizing the compose setup

# Available profiles:
# - default: Main services (backend + frontend)
# - dev: Development services with hot reload and shell access
# - localai: LocalAI-only service (no API layer)

# =============================================================================
# DEVELOPMENT & DEBUGGING
# =============================================================================

# Development Mode Settings
# -------------------------
# Set to true to enable development features
DEBUG_MODE=false

# Enable hot reload for backend (development only)
BACKEND_RELOAD=false

# Enable verbose logging for troubleshooting
VERBOSE_LOGGING=false

# =============================================================================
# SECURITY CONSIDERATIONS
# =============================================================================
# 
# IMPORTANT: This file contains configuration for your LLM container setup.
# 
# 1. Never commit .env files to version control
# 2. Use strong, unique values for any authentication tokens
# 3. Restrict network access to only necessary ports
# 4. Regularly update your models and dependencies
# 5. Monitor resource usage, especially for model storage
#
# For production deployments:
# - Use Docker secrets or external secret management
# - Enable HTTPS/TLS termination
# - Implement proper access controls
# - Set up monitoring and logging
# =============================================================================
