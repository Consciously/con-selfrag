services:
  # Main backend service: Ollama + FastAPI
  backend:
    build: 
      context: ./backend
      dockerfile: Dockerfile
    env_file:
      - .env
    ports:
      - "${MAIN_API_PORT:-8080}:${API_PORT:-8000}"    # FastAPI
      - "${MAIN_OLLAMA_PORT:-11434}:${OLLAMA_PORT:-11434}"  # Ollama
    environment:
      - OLLAMA_HOST=${OLLAMA_HOST:-0.0.0.0}
      - OLLAMA_PORT=${OLLAMA_PORT:-11434}
      - DEFAULT_MODEL=${DEFAULT_MODEL:-llama3.2:1b}
      - API_TIMEOUT=${API_TIMEOUT:-300}
      - LOG_LEVEL=${LOG_LEVEL:-INFO}
    volumes:
      - ${OLLAMA_MODELS_PATH:-/mnt/assets/models/llm}:/home/llmuser/.ollama/models
      - ./backend/scripts:/app/scripts:ro
    command: ["api"]
    restart: unless-stopped
    networks:
      - llm-network

  # Development backend service: Interactive shell with Ollama
  backend-dev:
    build: 
      context: ./backend
      dockerfile: Dockerfile
    env_file:
      - .env
    ports:
      - "${DEV_API_PORT:-8081}:${API_PORT:-8000}"    # Different port to avoid conflicts
      - "${DEV_OLLAMA_PORT:-11435}:${OLLAMA_PORT:-11434}"  # Different port to avoid conflicts
    environment:
      - OLLAMA_HOST=${OLLAMA_HOST:-0.0.0.0}
      - OLLAMA_PORT=${OLLAMA_PORT:-11434}
      - DEFAULT_MODEL=${DEFAULT_MODEL:-llama3.2:1b}
      - API_TIMEOUT=${API_TIMEOUT:-300}
      - LOG_LEVEL=${LOG_LEVEL:-INFO}
    volumes:
      - ${OLLAMA_MODELS_PATH:-/mnt/assets/models/llm}:/home/llmuser/.ollama/models
      - ./backend/scripts:/app/scripts
      - ./backend/app:/app/app
      - ./backend/api:/app/api
    command: ["shell"]
    stdin_open: true
    tty: true
    profiles: ["dev"]
    networks:
      - llm-network

  # Ollama-only service: Just the LLM backend
  ollama-only:
    build: 
      context: ./backend
      dockerfile: Dockerfile
    env_file:
      - .env
    ports:
      - "${OLLAMA_ONLY_PORT:-11436}:${OLLAMA_PORT:-11434}"  # Ollama only
    environment:
      - OLLAMA_HOST=${OLLAMA_HOST:-0.0.0.0}
      - OLLAMA_PORT=${OLLAMA_PORT:-11434}
      - DEFAULT_MODEL=${DEFAULT_MODEL:-llama3.2:1b}
      - LOG_LEVEL=${LOG_LEVEL:-INFO}
    volumes:
      - ${OLLAMA_MODELS_PATH:-/mnt/assets/models/llm}:/home/llmuser/.ollama/models
    command: ["ollama-only"]
    restart: unless-stopped
    profiles: ["ollama"]
    networks:
      - llm-network

networks:
  llm-network:
    driver: bridge
